{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bc8a514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\harim\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f997d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de3f4026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CITY</th>\n",
       "      <th>COUN</th>\n",
       "      <th>SUBJ</th>\n",
       "      <th>SEX</th>\n",
       "      <th>AGE</th>\n",
       "      <th>RELI</th>\n",
       "      <th>PRAC</th>\n",
       "      <th>FOCC</th>\n",
       "      <th>MOCC</th>\n",
       "      <th>...</th>\n",
       "      <th>SELF</th>\n",
       "      <th>RELA</th>\n",
       "      <th>VERBAL</th>\n",
       "      <th>NEUTRO</th>\n",
       "      <th>Field1</th>\n",
       "      <th>Field3</th>\n",
       "      <th>Field2</th>\n",
       "      <th>MYKEY</th>\n",
       "      <th>SIT</th>\n",
       "      <th>STATE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>joy</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>110011</td>\n",
       "      <td>During the period of falling in love, each tim...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>fear</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>110012</td>\n",
       "      <td>When I was involved in a traffic accident.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>anger</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>110013</td>\n",
       "      <td>When I was driving home after  several days of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>sadness</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>110014</td>\n",
       "      <td>When I lost the person who meant the most to me.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>disgust</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>110015</td>\n",
       "      <td>The time I knocked a deer down - the sight of ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID  CITY  COUN  SUBJ  SEX  AGE  RELI  PRAC  FOCC  MOCC  ...  SELF  RELA  \\\n",
       "0  11001     1     1     1    1   33     1     2     6     1  ...     3     3   \n",
       "1  11001     1     1     1    1   33     1     2     6     1  ...     2     2   \n",
       "2  11001     1     1     1    1   33     1     2     6     1  ...     2     1   \n",
       "3  11001     1     1     1    1   33     1     2     6     1  ...     1     1   \n",
       "4  11001     1     1     1    1   33     1     2     6     1  ...     0     2   \n",
       "\n",
       "   VERBAL  NEUTRO   Field1  Field3  Field2   MYKEY  \\\n",
       "0       2       0      joy       4       3  110011   \n",
       "1       0       0     fear       3       2  110012   \n",
       "2       0       0    anger       1       3  110013   \n",
       "3       0       2  sadness       4       4  110014   \n",
       "4       0       0  disgust       4       4  110015   \n",
       "\n",
       "                                                 SIT  STATE  \n",
       "0  During the period of falling in love, each tim...      1  \n",
       "1         When I was involved in a traffic accident.      1  \n",
       "2  When I was driving home after  several days of...      1  \n",
       "3  When I lost the person who meant the most to me.       1  \n",
       "4  The time I knocked a deer down - the sight of ...      1  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datadf=pd.read_csv(\"Data.csv\")\n",
    "datadf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c349e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data={\"sentence\":datadf[\"SIT\"],\"label\":datadf[\"Field1\"]}\n",
    "datadf=pd.concat(data,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b22613e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['joy', 'fear', 'anger', 'sadness', 'disgust', 'shame', 'guilt'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datadf[\"label\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39efc811",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict={\"joy\":1,\"fear\":2,\"anger\":3,\"sadness\":4,\"disgust\":5,\"shame\":6,\"guilt\":7}\n",
    "datadf[\"label\"]=datadf[\"label\"].map(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a13b8836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>During the period of falling in love, each tim...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When I was involved in a traffic accident.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>When I was driving home after  several days of...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>When I lost the person who meant the most to me.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The time I knocked a deer down - the sight of ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label\n",
       "0  During the period of falling in love, each tim...      1\n",
       "1         When I was involved in a traffic accident.      2\n",
       "2  When I was driving home after  several days of...      3\n",
       "3  When I lost the person who meant the most to me.       4\n",
       "4  The time I knocked a deer down - the sight of ...      5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datadf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92a2ae1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=datadf[:5000]\n",
    "test=datadf[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "177956e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.index.start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7fca637",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_example_to_feature(review):\n",
    "  return tokenizer.encode_plus(review,\n",
    "                add_special_tokens = True, # add [CLS], [SEP]\n",
    "                max_length = max_length, # max length of the text that can go to BERT\n",
    "                pad_to_max_length = True, # add [PAD] tokens\n",
    "                return_attention_mask = True, # add attention mask to not focus on pad tokens\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0903b25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 512\n",
    "batch_size = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "542c8d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_examples(ds):\n",
    "  # prepare list, so that we can build up final TensorFlow dataset from slices.\n",
    "  input_ids_list = []\n",
    "  token_type_ids_list = []\n",
    "  attention_mask_list = []\n",
    "  label_list = []\n",
    "  for i in range(ds.index.start,ds.index.stop):\n",
    "    review=ds[\"sentence\"][i]\n",
    "    label=ds[\"label\"][i]\n",
    "    bert_input = convert_example_to_feature(review)\n",
    "    input_ids_list.append(bert_input['input_ids'])\n",
    "    token_type_ids_list.append(bert_input['token_type_ids'])\n",
    "    attention_mask_list.append(bert_input['attention_mask'])\n",
    "    label_list.append([label])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29efcaa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harim\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2304: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ds_train_encoded = encode_examples(train).batch(batch_size)\n",
    "ds_test_encoded = encode_examples(test).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e98d38d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.batch_op._BatchDataset"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ds_train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc96d122",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "learning_rate = 2e-5\n",
    "number_of_epochs = 1\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.CategoricalCrossentropy('accuracy')\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b656c498",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\harim\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\harim\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\harim\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\harim\\anaconda3\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1529, in train_step\n        loss = self.compiled_loss(y, y_pred, sample_weight, regularization_losses=self.losses)\n    File \"C:\\Users\\harim\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\harim\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\harim\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\harim\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py\", line 2221, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"C:\\Users\\harim\\anaconda3\\lib\\site-packages\\keras\\src\\backend.py\", line 5573, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 2) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m bert_history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_train_encoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumber_of_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filedw9e8nk0.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\modeling_tf_utils.py:1529\u001b[0m, in \u001b[0;36mTFPreTrainedModel.train_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1526\u001b[0m             y_pred \u001b[38;5;241m=\u001b[39m y_pred[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1528\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1529\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompiled_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregularization_losses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlosses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;66;03m# Run backwards pass.\u001b[39;00m\n\u001b[0;32m   1532\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mminimize(loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainable_variables, tape\u001b[38;5;241m=\u001b[39mtape)\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\harim\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\harim\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\harim\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\harim\\anaconda3\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1529, in train_step\n        loss = self.compiled_loss(y, y_pred, sample_weight, regularization_losses=self.losses)\n    File \"C:\\Users\\harim\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\harim\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\harim\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\harim\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py\", line 2221, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"C:\\Users\\harim\\anaconda3\\lib\\site-packages\\keras\\src\\backend.py\", line 5573, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 2) are incompatible\n"
     ]
    }
   ],
   "source": [
    "bert_history = model.fit(ds_train_encoded, epochs=number_of_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c30c10a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_layer_norm', 'vocab_projector', 'vocab_transform', 'activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier', 'dropout_95', 'pre_classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\harim\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\harim\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\harim\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\harim\\anaconda3\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1495, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\harim\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\harim\\AppData\\Local\\Temp\\__autograph_generated_file451lzntn.py\", line 36, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n\n    TypeError: Exception encountered when calling layer 'tf_distil_bert_for_sequence_classification' (type TFDistilBertForSequenceClassification).\n    \n    in user code:\n    \n        File \"C:\\Users\\harim\\anaconda3\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1652, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n    \n        TypeError: outer_factory.<locals>.inner_factory.<locals>.tf__call() got an unexpected keyword argument 'token_type_ids'\n    \n    \n    Call arguments received by layer 'tf_distil_bert_for_sequence_classification' (type TFDistilBertForSequenceClassification):\n      â€¢ input_ids={'input_ids': 'tf.Tensor(shape=(None, None), dtype=int32)', 'attention_mask': 'tf.Tensor(shape=(None, None), dtype=int32)', 'token_type_ids': 'tf.Tensor(shape=(None, None), dtype=int32)'}\n      â€¢ attention_mask=None\n      â€¢ head_mask=None\n      â€¢ inputs_embeds=None\n      â€¢ output_attentions=None\n      â€¢ output_hidden_states=None\n      â€¢ return_dict=None\n      â€¢ labels=None\n      â€¢ training=True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-5\u001b[39m)\n\u001b[0;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptimizer, loss\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mcompute_loss)\n\u001b[1;32m----> 6\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_train_encoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filedw9e8nk0.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\modeling_tf_utils.py:1495\u001b[0m, in \u001b[0;36mTFPreTrainedModel.train_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1493\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(x, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1494\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1495\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_using_dummy_loss:\n\u001b[0;32m   1497\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompiled_loss(y_pred\u001b[38;5;241m.\u001b[39mloss, y_pred\u001b[38;5;241m.\u001b[39mloss, sample_weight, regularization_losses\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlosses)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file451lzntn.py:36\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__run_call_with_unpacked_inputs\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(func), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m),), \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(unpacked_inputs)), fscope)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     38\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: in user code:\n\n    File \"C:\\Users\\harim\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\harim\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\harim\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\harim\\anaconda3\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1495, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\harim\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\harim\\AppData\\Local\\Temp\\__autograph_generated_file451lzntn.py\", line 36, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n\n    TypeError: Exception encountered when calling layer 'tf_distil_bert_for_sequence_classification' (type TFDistilBertForSequenceClassification).\n    \n    in user code:\n    \n        File \"C:\\Users\\harim\\anaconda3\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1652, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n    \n        TypeError: outer_factory.<locals>.inner_factory.<locals>.tf__call() got an unexpected keyword argument 'token_type_ids'\n    \n    \n    Call arguments received by layer 'tf_distil_bert_for_sequence_classification' (type TFDistilBertForSequenceClassification):\n      â€¢ input_ids={'input_ids': 'tf.Tensor(shape=(None, None), dtype=int32)', 'attention_mask': 'tf.Tensor(shape=(None, None), dtype=int32)', 'token_type_ids': 'tf.Tensor(shape=(None, None), dtype=int32)'}\n      â€¢ attention_mask=None\n      â€¢ head_mask=None\n      â€¢ inputs_embeds=None\n      â€¢ output_attentions=None\n      â€¢ output_hidden_states=None\n      â€¢ return_dict=None\n      â€¢ labels=None\n      â€¢ training=True\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFDistilBertForSequenceClassification\n",
    "\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "model.compile(optimizer=optimizer, loss=model.compute_loss)\n",
    "model.fit(ds_train_encoded, epochs=3, batch_size=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66777eba",
   "metadata": {},
   "source": [
    "|good|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
